{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning TFlearn and Sentiment Analysis\n",
    "\n",
    "## Really just me playing around with TD learn and imdb data\n",
    "\n",
    "First step is to import the tdlearn libraries for usage, pretty easy right "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from tflearn.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we grab the tflearn inbuilt data set from IMDB, this will load the train/test with the first 10000 words and split the data 90/10 for training and testing data.\n",
    "\n",
    "The valid_portion=0.1 component keeps 10% of the training data for validation and testing of our hyperparameters\n",
    "\n",
    "The IMDB data set is a pkl (byte stream) file, the inbuild function just refrences a URL which is then downloaded dor usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data lenght:  2\n",
      "test data lenght:  2\n"
     ]
    }
   ],
   "source": [
    "# IMDB Dataset loading\n",
    "train, test, _ = imdb.load_data(path='imdb.pkl', n_words=10000,\n",
    "                                valid_portion=0.1)\n",
    "\n",
    "print(\"train data lenght: \", len(train))\n",
    "print(\"test data lenght: \", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first I thought these variables should show the total count, then I realised its splitting the data into two hence why two is returned......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data lenght:  22500\n",
      "sample train data:  [17, 25, 10, 406, 26, 14, 56, 61, 62, 323, 4]\n",
      "test data lenght:  2500\n",
      "sample test data:  [6, 694, 7, 19, 360, 19, 139, 33, 893, 8, 2567, 102, 760, 3, 2237, 5, 6803, 96, 17, 25, 10, 4]\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY = train\n",
    "testX, testY = test\n",
    "\n",
    "print(\"train data lenght: \", len(trainX))\n",
    "print(\"sample train data: \", trainX[0])\n",
    "print(\"test data lenght: \", len(testX))\n",
    "print(\"sample test data: \", testX[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that it's returning the total amount of items (length) and that we have a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated trainX[0]:  [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  17\n",
      "  25  10 406  26  14  56  61  62 323   4]\n",
      "updated testX[0]:  [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    6  694    7   19  360   19  139   33  893    8 2567  102\n",
      "  760    3 2237    5 6803   96   17   25   10    4]\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "# Sequence padding\n",
    "trainX = pad_sequences(trainX, maxlen=100, value=0.)\n",
    "testX = pad_sequences(testX, maxlen=100, value=0.)\n",
    "\n",
    "print(\"updated trainX[0]: \", trainX[0])\n",
    "print(\"updated testX[0]: \", testX[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have padded our trainX/testX input vectors into a uniform length of 100 with padding of 0 - now every item is the same lenght "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first entry trainY:  [0]\n",
      "first entry testY:  [0]\n",
      "updated trainY[0]:  [ 1.  0.]\n",
      "updated testY[0]:  [ 1.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"first entry trainY: \", trainY[:1])\n",
    "print(\"first entry testY: \", testY[:1])\n",
    "\n",
    "# Converting labels to binary vectors\n",
    "trainY = to_categorical(trainY, nb_classes=2)\n",
    "testY = to_categorical(testY, nb_classes=2)\n",
    "\n",
    "print(\"updated trainY[0]: \", trainY[0])\n",
    "print(\"updated testY[0]: \", testY[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we setup our labels, both test and train data, we end up with a binary class matrix for use with categorical_crossentropy, this is represneting a postivie or negative outcome of the assoicated review (no idea which is which at this point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network building\n",
    "net = tflearn.input_data([None, 100])\n",
    "net = tflearn.embedding(net, input_dim=10000, output_dim=128)\n",
    "net = tflearn.lstm(net, 128, dropout=0.8)\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build our network\n",
    "\n",
    "net = tflearn.input_data([None, 100]) sets up the input of data, we set the amount of input nodes or shape (remember earlier when we set our trainX to 100 with padding?) the first element is batch size, given it's a relativly small amount of data we set this to none\n",
    "\n",
    "net = tflearn.embedding(net, input_dim=10000, output_dim=128) this is our embedding layer, the input is the output of the first layer (net in this case) the input_dim is the amount of words we have loaded in from IMDB (we would change this based on how many words we have) and the output dimensions to 128 which is the number of dimensions in our results embeddings\n",
    "\n",
    "net = tflearn.lstm(net, 128, dropout=0.8) we now setup a long short term memory layer and feed in the outpu values of our embedding layer, we also setup a dropout rate which will randomly turn on and off pathways helping to ensure we dont overfit out data\n",
    "\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax') we then setup a fully_connected layer which means every neuron in the previous layer is connected to every neuron in this layer and set the activation function to softmax, adding a fully connected layer in a computationally cheap way of learning non-linear combinations of them\n",
    "\n",
    "softmax squashes the output values/probabilities for a range between 0 and 1 with a sum of 1\n",
    "\n",
    "net = tflearn.regression(net, optimizer='adam', learning_rate=0.001, loss='categorical_crossentropy') we then create our regression layer, this will apply a regression operation to the input, we specify an optimiser method (in this case adam) which will minimise a given loss function, we set our learning rate value, categorical_crossentropy is our loss function which finds the difference between our predicted output and the expected output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7040  | total loss: \u001b[1m\u001b[32m0.05335\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 010 | loss: 0.05335 - acc: 0.9913 | val_loss: 0.54796 - val_acc: 0.8456 -- iter: 22500/22500\n",
      "Training Step: 7040  | total loss: \u001b[1m\u001b[32m0.05335\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 010 | loss: 0.05335 - acc: 0.9913 | val_loss: 0.54796 - val_acc: 0.8456 -- iter: 22500/22500\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True,\n",
    "          batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With ten epochs the result was as above which is a very low loss (acc 99.13%!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
